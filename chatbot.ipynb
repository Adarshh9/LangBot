{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('data/hfData.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hugging Face, Inc. is a French-American company based in New York City that develops computation tools for building applications using machine learning. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.\\n\\nHistory\\nThe company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers.[1] The company was named after the \"hugging face\" emoji.[1] After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform for machine learning.\\n\\nIn March 2021, Hugging Face raised US$40 million in a Series B funding round.[2]\\n\\nOn April 28, 2021, the company launched the BigScience Research Workshop in collaboration with several other research groups to release an open large language model.[3] In 2022, the workshop concluded with the announcement of BLOOM, a multilingual large language model with 176 billion parameters.[4][5]\\n\\nIn December 2022, the company acquired Gradio, an open source library built for developing machine learning applications in Python.[6]\\n\\nOn May 5, 2022, the company announced its Series C funding round led by Coatue and Sequoia.[7] The company received a $2 billion valuation.\\n\\nOn August 3, 2022, the company announced the Private Hub, an enterprise version of its public Hugging Face Hub that supports SaaS or on-premises deployment.[8]\\n\\nIn February 2023, the company announced partnership with Amazon Web Services (AWS) which would allow Hugging Face\\'s products available to AWS customers to use them as the building blocks for their custom applications. The company also said the next generation of BLOOM will be run on Trainium, a proprietary machine learning chip created by AWS.[9][10][11]\\n\\nIn August 2023, the company announced that it raised $235 million in a Series D funding, at a $4.5 billion valuation. The funding was led by Salesforce, and notable participation came from Google, Amazon, Nvidia, AMD, Intel, IBM, and Qualcomm.[12]\\n\\nServices and technologies\\nTransformers Library\\nThe Transformers library is a Python package that contains open-source implementations of transformer models for text, image, and audio tasks. It is compatible with the PyTorch, TensorFlow and JAX deep learning libraries and includes implementations of notable models like BERT and GPT-2.[13] The library was originally called \"pytorch-pretrained-bert\"[14] which was then renamed to \"pytorch-transformers\" and finally \"transformers.\"\\n\\nHugging Face Hub\\nThe Hugging Face Hub is a platform (centralized web service) for hosting:[15]\\n\\nGit-based code repositories, including discussions and pull requests for projects.\\nmodels, also with Git-based version control;\\ndatasets, mainly in text, images, and audio;\\nweb applications (\"spaces\" and \"widgets\"), intended for small-scale demos of machine learning applications.\\nOther libraries\\n\\nGradio UI Example\\nIn addition to Transformers and the Hugging Face Hub, the Hugging Face ecosystem contains libraries for other tasks, such as dataset processing (\"Datasets\"), model evaluation (\"Evaluate\"), and machine learning demos (\"Gradio\").[16]', metadata={'source': 'data/hfData.txt'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_text = wrap_text_preserve_newlines(str(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'page_content=\\'Hugging Face, Inc. is a French-American company based in New York City that develops computation\\ntools for building applications using machine learning. It is most notable for its transformers library built\\nfor natural language processing applications and its platform that allows users to share machine learning\\nmodels and datasets and showcase their work.\\\\n\\\\nHistory\\\\nThe company was founded in 2016 by French\\nentrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company\\nthat developed a chatbot app targeted at teenagers.[1] The company was named after the \"hugging face\"\\nemoji.[1] After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform\\nfor machine learning.\\\\n\\\\nIn March 2021, Hugging Face raised US$40 million in a Series B funding\\nround.[2]\\\\n\\\\nOn April 28, 2021, the company launched the BigScience Research Workshop in collaboration with\\nseveral other research groups to release an open large language model.[3] In 2022, the workshop concluded with\\nthe announcement of BLOOM, a multilingual large language model with 176 billion parameters.[4][5]\\\\n\\\\nIn\\nDecember 2022, the company acquired Gradio, an open source library built for developing machine learning\\napplications in Python.[6]\\\\n\\\\nOn May 5, 2022, the company announced its Series C funding round led by Coatue\\nand Sequoia.[7] The company received a $2 billion valuation.\\\\n\\\\nOn August 3, 2022, the company announced the\\nPrivate Hub, an enterprise version of its public Hugging Face Hub that supports SaaS or on-premises\\ndeployment.[8]\\\\n\\\\nIn February 2023, the company announced partnership with Amazon Web Services (AWS) which\\nwould allow Hugging Face\\\\\\'s products available to AWS customers to use them as the building blocks for their\\ncustom applications. The company also said the next generation of BLOOM will be run on Trainium, a proprietary\\nmachine learning chip created by AWS.[9][10][11]\\\\n\\\\nIn August 2023, the company announced that it raised $235\\nmillion in a Series D funding, at a $4.5 billion valuation. The funding was led by Salesforce, and notable\\nparticipation came from Google, Amazon, Nvidia, AMD, Intel, IBM, and Qualcomm.[12]\\\\n\\\\nServices and\\ntechnologies\\\\nTransformers Library\\\\nThe Transformers library is a Python package that contains open-source\\nimplementations of transformer models for text, image, and audio tasks. It is compatible with the PyTorch,\\nTensorFlow and JAX deep learning libraries and includes implementations of notable models like BERT and\\nGPT-2.[13] The library was originally called \"pytorch-pretrained-bert\"[14] which was then renamed to \"pytorch-\\ntransformers\" and finally \"transformers.\"\\\\n\\\\nHugging Face Hub\\\\nThe Hugging Face Hub is a platform (centralized\\nweb service) for hosting:[15]\\\\n\\\\nGit-based code repositories, including discussions and pull requests for\\nprojects.\\\\nmodels, also with Git-based version control;\\\\ndatasets, mainly in text, images, and audio;\\\\nweb\\napplications (\"spaces\" and \"widgets\"), intended for small-scale demos of machine learning applications.\\\\nOther\\nlibraries\\\\n\\\\nGradio UI Example\\\\nIn addition to Transformers and the Hugging Face Hub, the Hugging Face\\necosystem contains libraries for other tasks, such as dataset processing (\"Datasets\"), model evaluation\\n(\"Evaluate\"), and machine learning demos (\"Gradio\").[16]\\' metadata={\\'source\\': \\'data/hfData.txt\\'}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hugging Face, Inc. is a French-American company based in New York City that develops computation tools for building applications using machine learning. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.\\n\\nHistory\\nThe company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers.[1] The company was named after the \"hugging face\" emoji.[1] After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform for machine learning.\\n\\nIn March 2021, Hugging Face raised US$40 million in a Series B funding round.[2]', metadata={'source': 'data/hfData.txt'}),\n",
       " Document(page_content='On April 28, 2021, the company launched the BigScience Research Workshop in collaboration with several other research groups to release an open large language model.[3] In 2022, the workshop concluded with the announcement of BLOOM, a multilingual large language model with 176 billion parameters.[4][5]\\n\\nIn December 2022, the company acquired Gradio, an open source library built for developing machine learning applications in Python.[6]\\n\\nOn May 5, 2022, the company announced its Series C funding round led by Coatue and Sequoia.[7] The company received a $2 billion valuation.\\n\\nOn August 3, 2022, the company announced the Private Hub, an enterprise version of its public Hugging Face Hub that supports SaaS or on-premises deployment.[8]', metadata={'source': 'data/hfData.txt'}),\n",
       " Document(page_content=\"In February 2023, the company announced partnership with Amazon Web Services (AWS) which would allow Hugging Face's products available to AWS customers to use them as the building blocks for their custom applications. The company also said the next generation of BLOOM will be run on Trainium, a proprietary machine learning chip created by AWS.[9][10][11]\\n\\nIn August 2023, the company announced that it raised $235 million in a Series D funding, at a $4.5 billion valuation. The funding was led by Salesforce, and notable participation came from Google, Amazon, Nvidia, AMD, Intel, IBM, and Qualcomm.[12]\", metadata={'source': 'data/hfData.txt'}),\n",
       " Document(page_content='Services and technologies\\nTransformers Library\\nThe Transformers library is a Python package that contains open-source implementations of transformer models for text, image, and audio tasks. It is compatible with the PyTorch, TensorFlow and JAX deep learning libraries and includes implementations of notable models like BERT and GPT-2.[13] The library was originally called \"pytorch-pretrained-bert\"[14] which was then renamed to \"pytorch-transformers\" and finally \"transformers.\"\\n\\nHugging Face Hub\\nThe Hugging Face Hub is a platform (centralized web service) for hosting:[15]\\n\\nGit-based code repositories, including discussions and pull requests for projects.\\nmodels, also with Git-based version control;\\ndatasets, mainly in text, images, and audio;\\nweb applications (\"spaces\" and \"widgets\"), intended for small-scale demos of machine learning applications.\\nOther libraries', metadata={'source': 'data/hfData.txt'}),\n",
       " Document(page_content='Gradio UI Example\\nIn addition to Transformers and the Hugging Face Hub, the Hugging Face ecosystem contains libraries for other tasks, such as dataset processing (\"Datasets\"), model evaluation (\"Evaluate\"), and machine learning demos (\"Gradio\").[16]', metadata={'source': 'data/hfData.txt'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh/Langchain-chatbot/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is hugging face?\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face, Inc. is a French-American company based in New York City that develops computation tools for\n",
      "building applications using machine learning. It is most notable for its transformers library built for\n",
      "natural language processing applications and its platform that allows users to share machine learning models\n",
      "and datasets and showcase their work.\n",
      "\n",
      "History\n",
      "The company was founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf in\n",
      "New York City, originally as a company that developed a chatbot app targeted at teenagers.[1] The company was\n",
      "named after the \"hugging face\" emoji.[1] After open sourcing the model behind the chatbot, the company pivoted\n",
      "to focus on being a platform for machine learning.\n",
      "\n",
      "In March 2021, Hugging Face raised US$40 million in a Series B funding round.[2]\n"
     ]
    }
   ],
   "source": [
    "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh/Langchain-chatbot/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id='google/flan-t5-large' ,model_kwargs={\"temperature\":0 ,\"max_length\":512})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceHub(client=<InferenceClient(model='google/flan-t5-large', timeout=None)>, repo_id='google/flan-t5-large', task='text2text-generation', model_kwargs={'temperature': 0, 'max_length': 512})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm ,chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh/Langchain-chatbot/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a French-American company based in New York City that develops computation tools for building applications using machine learning'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is hugging face?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs ,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'develops computation tools for building applications using machine learning'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What do hugging face company?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs ,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clément Delangue'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is the founder of hugging face company?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs ,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model/hf_database.pickle','wb') as file:\n",
    "    pickle.dump(db ,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/hf_chain.pickle','wb') as file:\n",
    "    pickle.dump(chain ,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
